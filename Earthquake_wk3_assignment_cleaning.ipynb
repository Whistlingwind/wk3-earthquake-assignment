{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Purpose of this file:\n",
    "\n",
    "- Handling of NaN values\n",
    "- Verifying if the data provided is adequate, and if not, add more relevant columns\n",
    "\n",
    "Summary\n",
    "\n",
    "NaN values -\n",
    "\n",
    "Looking at the values, there seems to be an understanding that majority of the updated values are from \"stronger\" earthquakes, which are detectable. And all the activity that is not \n",
    "in detectable range will be supplied NULL values, but also any logical failures are listed as NULL.\n",
    "\n",
    "It's not possible to determine the volume of \"weaker\" signals returning NULL, and logical error (Not with my current understanding of the calculations anyway). \n",
    "So it is safer to assume that majority of the signals are \"weak\", so I raised the floor from NULL to the lowest denominator across the board.\n",
    "\n",
    "Country Level Data\n",
    "\n",
    "I wanted to review an aggregated comparison across country/state level data, however, {place} value is not compatible for indexeing due to too much variability in the description/content.\n",
    "\n",
    "To enable country level aggregation, I used the reverse geocoding method, via Nominatim, which takes long and lat column data and returns the location data, according to the zoom level. \n",
    "Zoom 3 was used to bring back country level data.\n",
    "\n",
    "Manual sifting\n",
    "\n",
    "America being a very large country, resulted in the country looking like an outlier, being massively populated with events. So I extracted state level data and integrated that into the dataset to enable \n",
    "a clearer comparison and zoom level of where the events are occuring.\n",
    "\n",
    "(Don't run it, it takes 1-2 hours to run!)\n",
    "\n",
    "Results: (modified_all_month tab)\n",
    "https://raw.githubusercontent.com/Whistlingwind/wk3-earthquake-assignment/main/modified_all_month.csv\n",
    "\"\"\"\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import io\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.point import Point\n",
    "\n",
    "#Import Data and cleanse the NaN values from the dataset\n",
    "df = pd.read_csv(r'https://raw.githubusercontent.com/Whistlingwind/wk3-earthquake-assignment/main/all_month.csv', \n",
    "            index_col='id', \n",
    "            parse_dates=['time'],\n",
    "            header=0, \n",
    "            names=['time','Latitude','Longitude','depth','mag','magType','nst','gap','dmin','rms','net','id','updated','place','type','horizontalError','depthError',\n",
    "                   'magError','magNst','status','locationSource','magSource'] )\n",
    "\n",
    "#Replace any empty fields as NaN so it can be caught in the next steps\n",
    "df = df.replace('',np.nan) \n",
    "\n",
    "\"\"\"\n",
    "NaN replacement settings:\n",
    "\n",
    "nst : 0 - No specific floor other than 0\n",
    "gap : 0 - No specific floor other than 0\n",
    "dmin : 0 - No specific floor other than 0\n",
    "horizontalError : 0 - No specific floor other than 0\n",
    "depthError : 0 - No specific floor other than 0\n",
    "magError : 0 - No specific floor other than 0\n",
    "magNst : 1 - Denotes total number of stations for detection, assuming that at least 1 station is required for detection, so setting the floor as 1.\n",
    "\n",
    "\"\"\"\n",
    "#Create a collection of key value pairs (unordered), to determine the expected replacement for NaN values for those specified columns\n",
    "naFixValues = {\"nst\": 0, \"gap\": 0, \"dmin\": 0, \"horizontalError\": 0, \"depthError\": 0, \"magError\": 0, \"magNst\": 1}\n",
    "#Function to replace NaN values using numpy\n",
    "df.fillna(value=naFixValues)\n",
    "\n",
    "#Setup (assuming) crawler call, user agent identification as \"test\" to resolve access requirements\n",
    "geolocator = Nominatim(user_agent=\"test\")\n",
    "\n",
    "#Pass lat & long coordinates into reverse Nominatim function to return location data at zoom level 3 (country)\n",
    "def reverse_geocoding(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse(Point(lat, lon),zoom=3)\n",
    "        return location.raw['display_name']\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "#Call to reverse function, and store temp results into new column called \"country\"\n",
    "df['country'] = np.vectorize(reverse_geocoding)(df['Latitude'], df['Longitude'])\n",
    "\n",
    "#Save the result as CSV, mainly because it takes 70 minutes to process, and we can't be doing this mid-presentation or mid code/result review, as it would take forever.\n",
    "df.to_csv('all_month_modified.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       latitude   longitude            id\n",
      "0     38.825500 -122.854332    nc73941516\n",
      "1     58.177200 -155.255000  ak023c9xsxas\n",
      "2     55.910700 -158.895200  ak023c9xrwud\n",
      "3     61.043900 -148.400400  ak023c9xqda0\n",
      "4     63.514800 -151.058000  ak023c9xoh1i\n",
      "...         ...         ...           ...\n",
      "9480  13.837700  144.759200    us7000kse7\n",
      "9481  35.380167  -84.167167    se60546486\n",
      "9482  51.890667 -177.855000    av92020996\n",
      "9483  38.827835 -122.782837    nc73929141\n",
      "9484  19.379667 -155.284500    hv73544682\n",
      "\n",
      "[9485 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Whistlingwind\\Desktop\\Data Science\\Earthquake_data\\all_month.csv',header=0,usecols=['id','latitude','longitude'] )\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id   Latitude   Longitude\n",
      "0   1  30.197535  -97.662015\n",
      "1   2  34.895699  -82.218903\n",
      "2   3  33.636700  -84.428101\n",
      "3   4  33.636700  -84.428101\n",
      "4   5  32.733601 -117.190000\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'id': [1,2,3,4,5],\n",
    "                   'Latitude': [30.197535, 34.895699, 33.636700, 33.636700, 32.733601],\n",
    "                   'Longitude': [-97.662015, -82.218903, -84.428101, -84.428101, -117.190000]\n",
    "                    })\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
